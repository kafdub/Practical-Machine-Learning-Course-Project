---
title: "Practical Machine Learning Project"
author: "Patrick Sciamma"
date: "November 4, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Summary

The aim of the study is to predict if a physical activity is performed correctly.
We use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
We identify primary and statistical measures in our dataset and restrict our study to primary measures. Using 5-fold cross-validation we test three models: k-nearest neighbours, naive Bayes and random Forest. Based on an estimated accuracy of 99.29% we select the random forest model classifier, confirm its accuracy on our test set at above 99.29% and apply it to predict our final results.

## Source for the dataset
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz4y8Ou0ILz

## Data analysis

### load data and libraries

```{r load, results="hide", message = FALSE, warning=FALSE}
setwd("C:/Users/Patrick/Documents/R/MachineLearning/Project")
getwd()
library(caret)
library(dplyr)
library(magrittr)
library(Hmisc)
train <- read.csv("./data/pml-training.csv")
test <- read.csv("./data/pml-testing.csv")
```

### field selection

The training test contains 19622 observations of 160 variables.
The first fields are identifier, user name (6 users), time stamps, window identifier and 
indicator for new window

```{r quick_look}
dim(train)
dim(test)
str(train[,1:7])
```

From the data description at "http://groupware.les.inf.puc-rio.br/har"
we know the dataset contains measures for 4 sensors:
arm, forearm, belt and dumbbell

```{r fields}
names(train)[grep("belt", names(train))]
names(train)[grep("_arm", names(train))]
names(train)[grep("forearm", names(train))]
names(train)[grep("dumbbell", names(train))]
```

There are 13 primary measures for the belt:
roll, pitch, yaw, total acceleration, 'gyros', 'accel' and 'magnet'  measures on 3 axes

```{r measures}
beltm <- names(train)[grep("belt", names(train))]
i <- grep("(^[a-z]+_belt)|(^total_[a-z]+_belt)", beltm) 
beltm[i]
anyNA(train[,beltm[i]])
apply(train[,beltm[i]],2,function(x) sum(x == ''))
```

There are 25 statistics on those measures, with mostly NAs or errors when the new window indicator is False. The reasonable explanation is that for every new window, statistics are given for all measures in that window.

```{r statistics}
beltm[-i]
t <- train[train$new_window=='no',beltm[-i]]
str(t)
```


For example for window 2, avg_roll_belt contains average of all measures in that window.
```{r window2}
mean(train[train$num_window==2,'roll_belt'])
train[train$num_window==2 & train$new_window =='yes',]$avg_roll_belt
```

We will leave the statistics out as they are mostly redundant with the raw measure. We will revisit this choice if we find we don't have enough data after all

```{r select_fields}
train2 <- train %>% 
  select(-starts_with("kurtosis"),
         -starts_with("skewness"), 
         -starts_with("max"), 
         -starts_with("min"), 
         -starts_with("amplitude"), 
         -starts_with("avg"), 
         -starts_with("var"), 
         -starts_with("stddev"),
         -contains("timestamp"),
         -X,-new_window,-num_window
         )

test2 <- test %>% 
  select(-starts_with("kurtosis"),
         -starts_with("skewness"), 
         -starts_with("max"), 
         -starts_with("min"), 
         -starts_with("amplitude"), 
         -starts_with("avg"), 
         -starts_with("var"), 
         -starts_with("stddev"),
         -contains("timestamp"),
         -X,-new_window,-num_window,-problem_id
  )
```


The variable names are now consistent between train and test set, there are no NAs in our filtered dataset and a quick summary shows nothing untoward.

```{r data_check}
data.frame(names(train2[,1:53]), names(test2))
anyNA(train2)
anyNA(test2)
summary(train2)
summary(test2)
```

## Model Fit

### creation of training and a testing set

```{r trainingtesting}
set.seed(1342)
inTrain <- createDataPartition(y=train2$classe,
                               p=0.75, list=FALSE)
training <- train2[inTrain,]
testing <- train2[-inTrain,]
```


### set training parameters: we will run 5-fold cross validation
```{r fitcontrol}
fitControl <- trainControl(method = "cv", number=5)
```
### knn
We run a k-nearest neighbours models with 1, 2 and 5 neighbours

```{r knn, message = FALSE, warning=FALSE}
tuneGrid <-  expand.grid(k = c(1,2,5))
set.seed(32343)
modelFit <- train(classe ~. ,data=training, method="knn",
                  trControl = fitControl, tuneGrid=tuneGrid, preProcess=c("center", "scale"))
modelFit

```
The best result is achieved for k=1, with an accuracy of 98.66%

### naive Bayes
```{r naivebayes, message = FALSE, warning=FALSE}
library("naivebayes")
library("fastICA")
tuneGrid <-  expand.grid(fL = c(0,0.5,1),
                         usekernel = c(TRUE,FALSE),
                         adjust=c(TRUE,FALSE))

set.seed(32343)
modelFit <- train(classe ~. ,data=training, method="naive_bayes",
                  trControl = fitControl,  tuneGrid=tuneGrid)
modelFit
```

The best result is an accuracy of 73.6%

### random Forest
```{r random_Forest, message = FALSE, warning=FALSE}
library("randomForest")
rfGrid <-  expand.grid(mtry = 7)
set.seed(32343)
model <- train(classe~., data=training, trControl=fitControl, method="rf",
               tuneGrid=rfGrid, ntree=100)
model
model$finalModel
```

Accuracy is predicted at 99.31%, for an out of bag error estimate of 0.62%. This is our best model, let's double-check its accuracy on our test set

```{r test_set}
predictions <- predict.train(model,newdata=testing[,1:53])
confusionMatrix(predictions,testing$classe)

```

The results for the test set are consistent with the cross-validation, we now apply our model to the second set and create our submission.

```{r submission}
submission <- predict.train(model,newdata=test2)
submission
```
